import os
import time
import pickle
import requests
from selenium import webdriver
from selenium.webdriver.common.by import By
from selenium.webdriver.chrome.options import Options
import bs4

# **1. æµè§ˆå™¨é…ç½®**
options = Options()
options.headless = False
driver = webdriver.Chrome(options=options)

# **Pixiv ä¸»é¡µ**
pixiv_url = "https://www.pixiv.net/"
cookies_file = "cookies.pkl"

# **2. è¯»å– Cookies**
def load_cookies():
    driver.get(pixiv_url)
    time.sleep(1)
    if os.path.exists(cookies_file):
        with open(cookies_file, "rb") as f:
            cookies = pickle.load(f)
            for cookie in cookies:
                driver.add_cookie(cookie)
        driver.refresh()
        time.sleep(3)
        print("âœ… Cookies åŠ è½½æˆåŠŸï¼")
        return True
    return False

# **3. ç™»å½•å¹¶ä¿å­˜ Cookies**
def login_and_save_cookies():
    driver.get("https://accounts.pixiv.net/login")
    print("ğŸ”‘ è¯·æ‰‹åŠ¨ç™»å½• Pixiv...")
    time.sleep(30)  # é¢„ç•™æ—¶é—´

    cookies = driver.get_cookies()
    with open(cookies_file, "wb") as f:
        pickle.dump(cookies, f)
    print("âœ… ç™»å½•æˆåŠŸï¼ŒCookies å·²ä¿å­˜ï¼")

# **4. è‡ªåŠ¨ç™»å½•**
if not load_cookies():
    login_and_save_cookies()

# **5. è·å– Pixiv ä½œå“ç³»åˆ—çš„é“¾æ¥**
page_links = []

def get_pixiv_series_images(url):
    try:
        driver.get(url)
        time.sleep(1)

        for _ in range(1):
            driver.execute_script("window.scrollTo(0, document.body.scrollHeight);")
            time.sleep(1)

        soup = bs4.BeautifulSoup(driver.page_source, "html.parser")
        links = soup.find_all("a", class_="sc-d00cb9ca-0 jmvyWD sc-5a760b36-6 bsJslY gtm-gtm-manga-series-thumbnail")
        for link in links:
            href = link.get("href")
            if href not in page_links:
                page_links.append("https://www.pixiv.net" + href)
    except Exception as e:
        print(f"âš ï¸ é”™è¯¯: {e}")

for i in range(1, 5 + 1):
    url = f"https://www.pixiv.net/user/14601125/series/217742?p={i}#seriesContents"
    get_pixiv_series_images(url)
    print(f"âœ… å·²è·å–ç¬¬ {i} é¡µçš„é“¾æ¥")
    time.sleep(1)

print("âœ… æ‰€æœ‰ä½œå“é¡µé¢é“¾æ¥å·²è·å–ï¼")

# **6. ä½¿ç”¨ requests ä¸‹è½½å›¾ç‰‡**
def download_image(url, filename):
    headers = {
        "Referer": "https://www.pixiv.net/",
        "User-Agent": "Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/110.0.0.0 Safari/537.36"
    }
    
    session = requests.Session()
    for cookie in driver.get_cookies():
        session.cookies.set(cookie["name"], cookie["value"])
    
    response = session.get(url, headers=headers, stream=True)
    
    if response.status_code == 200:
        with open(filename, "wb") as f:
            for chunk in response.iter_content(1024):
                f.write(chunk)
        print(f"âœ… ä¸‹è½½å®Œæˆ: {filename}")
    else:
        print(f"âŒ ä¸‹è½½å¤±è´¥: {url}, çŠ¶æ€ç : {response.status_code}")

# **7. è·å–ä½œå“é¡µé¢çš„æ‰€æœ‰å›¾ç‰‡**
def get_image_links(url, save_path):
    print(f"ğŸ“Œ è®¿é—®é¡µé¢: {url}")
    driver.get(url)
    time.sleep(1)

    try:
        driver.find_element(By.XPATH, '//*[@id="root"]/div[2]/div/div[3]/div/div/div[1]/main/section/div[1]/div/div[5]/div/div/button/div[2]').click()
        time.sleep(1)
    except Exception:
        print("âš ï¸ æ²¡æœ‰æ‰¾åˆ° 'é˜…è¯»ä½œå“' æŒ‰é’®ï¼Œå¯èƒ½è¯¥é¡µé¢ä¸éœ€è¦ç‚¹å‡»")

    for _ in range(1):  
        driver.execute_script("window.scrollTo(0, document.body.scrollHeight);")
        time.sleep(1)

    soup = bs4.BeautifulSoup(driver.page_source, "html.parser")
    links = soup.find_all("a", class_="gtm-expand-full-size-illust")

    if not links:
        print("âš ï¸ æ²¡æœ‰æ‰¾åˆ°å›¾ç‰‡é“¾æ¥ï¼Œå¯èƒ½éœ€è¦è°ƒæ•´ XPATH æˆ–è€…æ‰‹åŠ¨æ£€æŸ¥ HTML ç»“æ„")
        return

    print(f"ğŸ” å‘ç° {len(links)} å¼ å›¾ç‰‡ï¼Œå¼€å§‹ä¸‹è½½...")

    # **åˆ›å»ºç›®å½•**
    if not os.path.exists(save_path):
        os.makedirs(save_path)

    for i, link in enumerate(links):
        img_url = link.get("href")
        filename = os.path.join(save_path, img_url.split("/")[-1])

        download_image(img_url, filename)
        time.sleep(1)

    print(f"âœ… ç« èŠ‚ {save_path} ä¸‹è½½å®Œæˆï¼")

# **8. å¤„ç†æ‰€æœ‰ä½œå“é¡µé¢**
total_chapters = len(page_links)
for i, link in enumerate(page_links):
    chapter_number = total_chapters - i + 1  # è®©æœ€æ—©çš„ç« èŠ‚æ’åœ¨æœ€å‰
    save_folder = f"Blue Archive/Chapter {chapter_number}"
    
    get_image_links(link, save_folder)

driver.quit()
